# HTTP 완벽 가이드 9장 - 웹 로봇

# 웹 로봇

사람과의 상호작용 없이, 연속된 웹 트렌잭션들을 자동으로 수행하는 소프트웨어 프로그램 

크롤러, 스파이더, 웜, 봇 등으로 불린다.

<br /><br />

## 9.1 크롤러와 크롤링

특정 URL들의 집합인 **루트 집합에서 방문을 시작**하여, **만나는 모든 문서를 끌어오는 봇**

끌어와진 문서들은 검색 가능한 데이터베이스로 변환된다. 

검색한 각 페이지들을 HTML 파싱 > URL 링크들 추출 > 상대링크를 절대링크로 변환하는 작업을 거친다.

> **좋은 루트 집합**
크고 인기있는 웹 사이트, 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성
>


<br />

### 순환 피하기

로봇이 웹을 크롤링할 때 , 루프나 순환에 빠지지 않도록 매우 조심해야 한다. 

순환에 빠지지 않기 위해, 로봇들은 각자가 방문한 위치를 기억해야 한다.

![image](https://github.com/musinsa-global-developer-study/HTTP-The-definitive-guide/assets/72294509/63953ea9-8214-428c-8484-8c8ec0388c50)

<br />

### 빵 부스러기(?)의 흔적

크롤러가 각자가 방문한 위치를 기억하는 데에는 몇 가지 방법이 있다.

1. 트리와 해시 테이블 
2. 느슨한 존재 비트맵
    1. 공간 사용을 최소화하기 위해, 몇몇 대규모 크롤러들은 존재 비트 배열(presence bit array)와 같은 느슨한 자료 구조를 사용한다. 
3. 체크포인트
    1. 방문한 URL의 목록이 디스크에 잘 저장되었는지 중간에 확인
4. 파티셔닝
    1. 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 농장을 이용하고 있는 대규모 크롤러들이 있다.
    2. 각 로봇에는 URL의 특정 ‘한 부분’이 할당되어 그 부분에 대한 책임을 진다. 

<br />

### URL 정규화

대부분의 웹 로봇은 URL을 표준 형식으로 정규화 함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도한다. (중복 제거)

1. 포트 번호가 명시되지 않았다면 호스트 명에 `:80`을 추가한다
2. 모든 `%xx` 이스케이핑 된 문자들을 대응되는 문자로 변환한다.
3. `#` 태그들을 제거한다.

<br />

### 파일 시스템 링크 순환

![image](https://github.com/musinsa-global-developer-study/HTTP-The-definitive-guide/assets/72294509/2f228c9b-84b1-449e-937c-2b07b06c8a67)

**(a)의 경우**

- /index.html 을 가져와서, subdir/index.html 발견함.
- subdir/index.html 가져와서, subdir/logo.gif로 이어지는 링크 발견
- subdir/logo.gif을 가져오고 더 이상 링크가 없으므로 완료

**(b)의 경우**

- /index.html 을 가져와서, subdir/index.html 발견함.
- subdir/index.html을 가져왔지만 같은 index.html로 되돌아감
- subdir/subdir/index.html을 가져온다
- subdir/subdir/subdir/index.html을 가져오고 무한반복한다.

<br />

### 동적 가상 웹 공간

웹 마스터들이 악의적으로 로봇을 크롤러 루프에 빠트릴 수 있다. 

또한 같은 서버에 있는 가상의 URL에 대한 링크를 포함한 HTML을 즉석에서 만들어내어, 로봇을 해당 페이지 루프에 빠트리게 할 수도 있다. 

더 흔하게, 웹 마스터는 악의를 가지지 않았음에도 의도치 않게 심벌릭 링크나 동적 콘텐츠로 크롤러 함정을 만들 수 있다. 

<br />

### 순환을 피하는 방법

이렇듯 흔하게 일어날 수 있는 순환을 피하기 위해 로봇은 다음과 같은 방법을 사용한다.

1. URL 정규화
2. 너비 우선 크롤링
    - 방문할 URL들을 웹 사이트 전체에 걸쳐 너비 우선으로 스케줄링할 경우, 순환의 영향을 최소화할 수 있다.
3. 스로틀링
    - 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한한다.
4. URL 크기 제한
    - 일정 길이를 넘는 URL의 크롤링은 거부한다.
    - 이 기법을 적용할 경우, 가져오지 못하는 콘텐츠 또한 생길 수 있으므로 로깅 용으로 주로 사용한다.
5. URL/사이트 블랙리스트
6. 패턴 발견
    - URL의 중복된 요소로 잠재적인 순환으로 분류한다.
7. 콘텐츠 지문 (fingerprint)
    - 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬(checksum)을 계산하여, 해당 기준으로 중복을 판단한다.
        - 체크섬 : 페이지 내용의 간략한 표현
8. 사람의 모니터링


<br /><br />

## 9.2 로봇의 HTTP

로봇 역시 HTTP 명세 규칙을 지켜야 한다. (HTTP 요청 헤더 사용)

주로 HTTP/1.0 요청을 보낸다. 

<br />

### 요청 헤더 식별

웹로봇은 신원 식별 헤더를 구현하고 전송한다.

**그 이유는?** 
잘못된 크롤러의 소유자를 찾아낼 때와, 서버에게 로봇이 어떤 종류의 콘텐츠를 다룰 수 있는지에 대한 
약간의 정보를 줄 때 유용하기 때문이다.

권장되는 기본 신원 식별 헤더

- User-Agent (요청을 만든 로봇의 이름)
- From (로봇의 사용자 또는 관리인의 이메일 주소)
- Accept (어떤 미디어 타입을 보내도 되는지 명시)
- Referer (현재의 요청 URL을 포함한 문서의 URL 제공)

<br />

### 가상 호스팅

로봇 구현자들은 **Host** 헤더를 지원할 필요가 있다. 

요청에 Host 헤더를 포함하지 않으면 **로봇이 URL에 대해 잘못된 콘텐츠를 찾게 만든다.**

<br />

두 개의 사이트([www.joes.hardware.com](http://www.joes.hardware.com/), www.foo.com)를 운영하는 서버에 
host 헤더를 포함하지 않고 요청을 보냈을 때, 

서버가 기본적으로 [www.joes.hardware.com](http://www.joes.hardware.com/) 를 제공하도록 설정되어 있다면, 
www.foo.com페이지에 대한 요청은 www.joes.hardware.com에 대한 콘텐츠를 얻게 되고, **크롤러는 이를 구분할 수 없다.**

![image](https://github.com/musinsa-global-developer-study/HTTP-The-definitive-guide/assets/72294509/7fae3e45-6574-4cb5-abb5-d7404e27bd80)

<br />

### 조건부 요청

HTTP 캐시가 전에 받아온 리소스의 로컬 사본 유효성을 검사하는 방법과 매우 비슷하게, 

로봇이 받아간 마지막 버전 이후에 업데이트 된 부분이 있는지 알아보는 조건부 HTTP 요청을 구현할 수 있다. 

<br />

### 응답 다루기

GET 메서드 이외의 특정 기능을 사용하는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄 알아야 한다.

- 상태 코드
- 엔터티
    - HTTP 헤더에 임베딩한 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있다.

<br />

### User-Agent 타겟팅

사이트 관리자들은 자신의 사이트에 방문하게 될 로봇들의 요청을 예상하고 전략을 세워야 할 필요가 있다.

<br /><br />

## 9.3 부적절한 로봇들

로봇들이 저지르는 실수와 그로 인해 초래되는 결과는 다음과 같다.

<br />

### 폭주하는 로봇

로봇이 엄청나게 빠른 속도의 HTTP 요청을 만들고 이 요청이 에러를 갖고 있거나, 로봇이 순환에 빠지게 된다면
웹 서버에 극심한 부하를 안겨줄 수 있으며 그 누구도 서비스를 사용하지 못하게 만들 수 있다.

<br />

### 오래된 URL

로봇들이 오래되어 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있다. 

이는 존재하지 않는 문서에 대한 접근 요청으로 에러 로그가 채워지거나, 에러 페이지를 제공하는 부하로 문제가 발생할 수 있다. 

<br />

### 길고 잘못된 URL

웹 사이트에게 크고 의미없는 URL을 요청할 수 있다. 

이는 웹 서버의 처리 능력에 영향을 주고, 수많은 접근 에러 로그를 출력한다.

<br />

### 호기심이 지나친 로봇

몇몇 로봇들은 사적인 데이터를 얻어, 그 데이터를 인터넷 검색 엔진이나 기타 어플리케이션을 통해 접근할 수 있도록 만들 수 있다. 

만약 그 데이터의 소유자가 이를 원치 않았다면, 사생활 침해로 여길 수 ㅣㅇㅆ다. 

따라서 웹 소유자들은 이를 주의하여 개발해야 한다.

<br />

### 동적 게이트웨이 접근

로봇은 게이트웨이 어플리케이션 콘텐츠에 대한 URI로 요청을 할 수 있다. 

이 경우 얻은 데이터는 아마도 특수 목적을 위한 것일 테고, 처리 비용이 많이 들어 많은 웹 소유자들이 선호하지 않는다.

<br /><br /><br />

## 9.4 로봇 차단

로봇이 웹 서버에 요청할 때 페이지를 가져올 수 있는 권한이 있는지 확인하기 위해 `robot.txt`파일을 가져와서 확인한다.

![image](https://github.com/musinsa-global-developer-study/HTTP-The-definitive-guide/assets/72294509/7c27c8c3-c44a-4a59-a8c6-8adcc2a86431)

<br />

### robots.txt

웹 사이트의 어떤 URL을 방문하기 전에, 그 웹 사이트에 robots.txt파일이 존재한다면 
로봇은 반드시 그 파일을 가져와 처리해야 한다.

- 로봇은 웹 서버의 여느 파일들과 동일하게 HTTP `GET` 메서드를 이용해 robots.txt 리소스를 가져온다.
- robot.txt 파일이 있는 경우(200 ok) 그 제약조건에 따라서 로봇이 동작하고, 
없는 경우(http 404 nost found) 제약 없이 사이트에 접근한다.
- `401`또는 `403`응답을 수신한 경우, 로봇은 그 사이트로부터 접근이 완전히 제한되어있다고 가정해야한다.
- 서버 응답이 `3xx` 리다이렉션을 의미하면 로봇은 리소스가 발견될 때까지 리다이렉트를 따라가야 한다.
- 로봇은 자신이 이해하지 못하는 robots.txt 필드는 무시한다.
- 로봇은 주기적으로 robots.txt 파일을 가져와 그 결과를 캐시한다.
    - Cache-Control, Expires 헤더 명세가 중요

<br />

- **상용 웹 로봇이 보낼 수 있는 HTTP 크롤러 요청 예시**

```
GET /robots.txt HTTP/1.0
Host: www.joes-hardware.com
User-Agent: Slurp/2.0
Date: Wed Oct 3 20:22:48 EST 2001
```

<br />

- **robots.txt 예시**

```
# slurp, webcrawler가 우리 사이트의 공개된 영역을 크롤링하는 것을 허용 (다른 로봇 불가)

User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```

- `User-Agent` : 로봇의 이름
- `Disallow` : 특정 로봇에 대해 금지된 경로
- `Allow` : 특정 로봇에 대해 허용된 경로

<br />

### 로봇 차단 펄 코드

robots.txt와 상호작용하는 몇가지 펄(Perl) 라이브러리가 존재한다.

로봇이 주어진 URL에 대해 접근이 금지되어 있는지 확인할 수 있는 메서드를 제공한다. 

<br />

### 로봇 제어 META 태그

HTML 문서에 직접 META 태그를 추가함으로써 로봇을 제어할 수 있다. 

```html
<META NAME="ROBOTS" CONTENT="NOINDEX"> // 이 페이지를 처리하지 말고 무시
<META NAME="ROBOTS" CONTENT="NOFOLLOW"> // 이 페이지가 링크한 페이지를 무시
<META NAME="ROBOTS" CONTENT="INDEX"> // 이 페이지의 콘텐츠를 인덱싱해도 됨
<META NAME="ROBOTS" CONTENT="FOLLOW"> // 이 페이지가 링크한 페이지를 크롤링해도 됨
<META NAME="ROBOTS" CONTENT="NOARCHIVE"> // 이 페이지의 캐시를 만들어서는 안됨
<META NAME="ROBOTS" CONTENT="ALL"> // ALL=INDEX + FOLLOW
<META NAME="ROBOTS" CONTENT="NONE"> // NOINDEX + NOFOLLOW
```

<br />

### 검색엔진 META 태그

위의 태그들을 포함하여 더 많은 META 태그들을 사용할 수 있다.

밑의 태그들은 검색엔진 로봇들에 대해 유용하다.

```html
<META NAME="DESCRIPTION" CONTENT="<텍스트>"> // 웹페이지의 짧은 요약 정의
<META NAME="KEYWORKD" CONTENT="<쉼표 목록>"> // 웹 페이지를 기술하는 단어 목록 (쉼표 구분)
<META NAME="REVISIT-AFTER" CONTENT="<숫자 days>">
	// 로봇에게 이 페이지는 쉽게 변경될 것이기 때문에, <숫자 days> 뒤에 방문해달라고 지시
```

<br /><br />

## 9.5 로봇 에티켓

로봇 텍스트를 작성하기 위한 가이드라인이 존재한다.

[The Web Robots Pages](https://www.robotstxt.org/guidelines.html)

<br /><br />

## 9.6 검색엔진

검색엔진에서 웹 로봇을 가장 광범위하게 사용한다. 

웹 크롤러가 문서들을 끌어와서 검색 엔진이 어떤 문서에 어떤 단어들이 존재하는 지에 대한 색인 생성을 할 수 있게 돕는다. 

<br />

### 현대적인 검색엔진의 아키텍처

오늘날 검색엔진들은 ‘**풀 텍스트 색인(full-text indexes)**’라고 하는 복잡한 로컬 데이터베이스를 생성하고, 이 색인은 웹의 모든 문서에 대한 카드 카탈로그처럼 동작한다. 

![image](https://github.com/musinsa-global-developer-study/HTTP-The-definitive-guide/assets/72294509/729f0932-07c1-40f3-bae6-81fdebf195a5)

<br />

### 풀 텍스트 색인

풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스이다. 

![image](https://github.com/musinsa-global-developer-study/HTTP-The-definitive-guide/assets/72294509/c14b0252-ea06-4767-a2fe-76e84df35081)

<br />

### 질의 보내기

사용자는 HTML 폼을 채우고, 브라우저가 해당 폼을 GET, POST 요청을 이용하여 게이트웨이로 보냄으로써 질의를 웹 검색엔진 게이트웨이로 보낼 수 있다. 

![image](https://github.com/musinsa-global-developer-study/HTTP-The-definitive-guide/assets/72294509/e9074bca-cf64-485f-811b-647a23a870d2)

<br />

### 검색 결과 정렬 및 보여주기

검색을 했을 때 해당 단어와 관련이 많은 순서대로 결과에 나타낼 수 있도록 순서를 결정한다.

- 관련도 알고리즘을 사용한 관련도 랭킹 매기기

이를 위해 많은 검색 엔진은 웹 크롤링에서 얻은 정보(ex. 해당 페이지를 가리키는 링크들의 수)를 활용한다.

- 대부분 웹 사이트 운영자들은 이런 알고리즘을 이용해 자신의 페이지를 상위 랭크 시키려 노력하곤 한다.

<br />

### 스푸핑

웹 운영자들은 자신의 웹페이지를 눈에 더 잘 띄게 만들고 싶어한다. 따라서 다음과 같은 속임수를 쓸 수 있다.

- 수많은 키워드들을 나열한 가짜 페이지 생성
- 검색 엔진의 관련도 알고리즘을 더 잘 속일 수 있는 특정 단어에 대한 가짜 페이지를 생성하는 게이트웨이 어플리케이션 사용

따라서 검색엔진과 로봇 개발자들은 이러한 속임수들을 더 잘 잡아내기 위해 끊임없이 관련도 알고리즘을 수정헤야 한다.
